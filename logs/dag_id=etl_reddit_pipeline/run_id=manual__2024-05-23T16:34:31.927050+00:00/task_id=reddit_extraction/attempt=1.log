[2024-05-23T16:34:33.324+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-05-23T16:34:33.343+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-05-23T16:34:31.927050+00:00 [queued]>
[2024-05-23T16:34:33.353+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-05-23T16:34:31.927050+00:00 [queued]>
[2024-05-23T16:34:33.353+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-05-23T16:34:33.368+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-05-23 16:34:31.927050+00:00
[2024-05-23T16:34:33.373+0000] {standard_task_runner.py:63} INFO - Started process 72 to run task
[2024-05-23T16:34:33.377+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2024-05-23T16:34:31.927050+00:00', '--job-id', '14', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmp5va5qp0g']
[2024-05-23T16:34:33.380+0000] {standard_task_runner.py:91} INFO - Job 14: Subtask reddit_extraction
[2024-05-23T16:34:33.439+0000] {task_command.py:426} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-05-23T16:34:31.927050+00:00 [running]> on host f174faf60ef4
[2024-05-23T16:34:33.533+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='OlivierAssiene' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-05-23T16:34:31.927050+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-05-23T16:34:31.927050+00:00'
[2024-05-23T16:34:33.533+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-05-23T16:34:33.548+0000] {logging_mixin.py:188} INFO - 488KT1jGl8wGiX_3NB-G2A oC2c5DNFlETyHNlD-I9QtvM2Fs8wLg my_pipeline2 v0.1
[2024-05-23T16:34:33.549+0000] {logging_mixin.py:188} INFO - Connected successfully to Reddit!
[2024-05-23T16:34:34.376+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Currently work at a place where data is a cost center and it sucks. Iâ€™ve been applying for work again and iâ€™m wondering if i should avoid large non-tech companies.\n\nSome issues i face right now:\n- Terrible data warehouse. I canâ€™t even call it a data warehouse because data modeling is that bad\n\n- Lack of a data architect and sr data engineers. Design patterns, standards, tools, knowledge, and processes are all questionable\n\n- Majority of resources are offshore contractors from sweatshop companies\n\n- Non-technical executive. This seems to be the root cause of why everything is terrible. The org was definitely designed by someone whose never done this work\n\n- Micromanagement culture. Progress is measured by output and not outcomes\n\n- Lots of politics', 'author_fullname': 't2_5t9ezkftx', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is it common for large non-tech companies to be incompetent? Should i avoid them?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cyl5i2', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 61, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 61, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716441706.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Currently work at a place where data is a cost center and it sucks. Iâ€™ve been applying for work again and iâ€™m wondering if i should avoid large non-tech companies.</p>\n\n<p>Some issues i face right now:\n- Terrible data warehouse. I canâ€™t even call it a data warehouse because data modeling is that bad</p>\n\n<ul>\n<li><p>Lack of a data architect and sr data engineers. Design patterns, standards, tools, knowledge, and processes are all questionable</p></li>\n<li><p>Majority of resources are offshore contractors from sweatshop companies</p></li>\n<li><p>Non-technical executive. This seems to be the root cause of why everything is terrible. The org was definitely designed by someone whose never done this work</p></li>\n<li><p>Micromanagement culture. Progress is measured by output and not outcomes</p></li>\n<li><p>Lots of politics</p></li>\n</ul>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cyl5i2', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Usr-unkwn'), 'discussion_type': None, 'num_comments': 33, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyl5i2/is_it_common_for_large_nontech_companies_to_be/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyl5i2/is_it_common_for_large_nontech_companies_to_be/', 'subreddit_subscribers': 185337, 'created_utc': 1716441706.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.376+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I hit publish on a blogpost last week on running Spark, Dask, DuckDB, and Polars on the TPC-H benchmark across a variety of scales (10 GiB, 100 GiB, 1 TiB, 10 TiB), both locally on a Macbook Pro and on the cloud.\xa0 Itâ€™s a broad set of configurations.\xa0 The results are interesting.\n\nNo project wins uniformly.\xa0 They all perform differently at different scales:\xa0\n\n* DuckDB and Polars are crazy fast on local machines\n* Dask and DuckDB seem to win on cloud and at scale\n* Dask ends up being most robust, especially at scale\n* DuckDB does shockingly well on large datasets on a single large machine\n* Spark performs oddly poorly, despite being the standard choice ðŸ˜¢\n\nTons of charts in this post to try to make sense of the data.\xa0 If folks are curious, hereâ€™s the post:\n\n[https://docs.coiled.io/blog/tpch.html](https://docs.coiled.io/blog/tpch.html)\n\nPerformance isnâ€™t everything of course.\xa0 Each project has its die-hard fans/critics for loads of different reasons.  Anyone want to attack/defend their dataframe library of choice?', 'author_fullname': 't2_ay1q1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'TPC-H Cloud Benchmarks: Spark, Dask, DuckDB, Polars', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cyqe2z', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 35, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 35, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716463312.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I hit publish on a blogpost last week on running Spark, Dask, DuckDB, and Polars on the TPC-H benchmark across a variety of scales (10 GiB, 100 GiB, 1 TiB, 10 TiB), both locally on a Macbook Pro and on the cloud.\xa0 Itâ€™s a broad set of configurations.\xa0 The results are interesting.</p>\n\n<p>No project wins uniformly.\xa0 They all perform differently at different scales:\xa0</p>\n\n<ul>\n<li>DuckDB and Polars are crazy fast on local machines</li>\n<li>Dask and DuckDB seem to win on cloud and at scale</li>\n<li>Dask ends up being most robust, especially at scale</li>\n<li>DuckDB does shockingly well on large datasets on a single large machine</li>\n<li>Spark performs oddly poorly, despite being the standard choice ðŸ˜¢</li>\n</ul>\n\n<p>Tons of charts in this post to try to make sense of the data.\xa0 If folks are curious, hereâ€™s the post:</p>\n\n<p><a href="https://docs.coiled.io/blog/tpch.html">https://docs.coiled.io/blog/tpch.html</a></p>\n\n<p>Performance isnâ€™t everything of course.\xa0 Each project has its die-hard fans/critics for loads of different reasons.  Anyone want to attack/defend their dataframe library of choice?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1cyqe2z', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='mrocklin'), 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyqe2z/tpch_cloud_benchmarks_spark_dask_duckdb_polars/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyqe2z/tpch_cloud_benchmarks_spark_dask_duckdb_polars/', 'subreddit_subscribers': 185337, 'created_utc': 1716463312.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.376+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi would appreciate if someone could share wisdom about this\n', 'author_fullname': 't2_u7fsj6m26', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What Airflow does that a simple python script cannot do?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cyaqox', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.71, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 24, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 24, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716410501.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi would appreciate if someone could share wisdom about this</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cyaqox', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ubiond'), 'discussion_type': None, 'num_comments': 29, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyaqox/what_airflow_does_that_a_simple_python_script/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyaqox/what_airflow_does_that_a_simple_python_script/', 'subreddit_subscribers': 185337, 'created_utc': 1716410501.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.377+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Notes:\n\n1. Dashboards aren't done in Metabase, I have a lot to learn about SQL and I'm sure it could be argued I should have spent more time learning these fundamentals. \n\n2. Let's imagine there are three ways to get things done, regarding my code: copy/paste from online search or Stack Overflow, copy/paste from ChatGPT, writing manually. Do you see there being a difference in copying from SO and ChatGPT? If you were getting started today, how would you balance learning and utilizing ChatGPT? I'm not trying to argue against learning to do it manually, I would just like to know how professionals are using ChatGPT in the real world. I'm sure I relied on it too heavily, but I really wanted to get through this first project and get exposure. I learned a lot. \n\n3. I used ChatGPT to extract data from a PDF. What are other popular tools to do this? \n\n4. This is my first project. Do you think I should change anything before sharing? Will I get laughed at for using ChatGPT at all? \n\nI'm not out here trying to cut corners, and appreciate any insight. I just want to make you guys proud. \n\nHoping the next project will be simpler - I ran into so many roadblocks with the Energy API and port forwarding on my own network, due to a conflict with pfsense and my access point that was still behaving as a router, apparently. \n\nThanks in advance", 'author_fullname': 't2_ybtyi59a8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'First project update: complete, few questions. Please be critical. ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 103, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cya2ch', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'ups': 25, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Personal Project Showcase', 'can_mod_post': False, 'score': 25, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/Sl9KI-1IcAkO6DgO6qmpnzhSYHCAjaxKYlxzl2A8ZcA.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1716408744.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Notes:</p>\n\n<ol>\n<li><p>Dashboards aren&#39;t done in Metabase, I have a lot to learn about SQL and I&#39;m sure it could be argued I should have spent more time learning these fundamentals. </p></li>\n<li><p>Let&#39;s imagine there are three ways to get things done, regarding my code: copy/paste from online search or Stack Overflow, copy/paste from ChatGPT, writing manually. Do you see there being a difference in copying from SO and ChatGPT? If you were getting started today, how would you balance learning and utilizing ChatGPT? I&#39;m not trying to argue against learning to do it manually, I would just like to know how professionals are using ChatGPT in the real world. I&#39;m sure I relied on it too heavily, but I really wanted to get through this first project and get exposure. I learned a lot. </p></li>\n<li><p>I used ChatGPT to extract data from a PDF. What are other popular tools to do this? </p></li>\n<li><p>This is my first project. Do you think I should change anything before sharing? Will I get laughed at for using ChatGPT at all? </p></li>\n</ol>\n\n<p>I&#39;m not out here trying to cut corners, and appreciate any insight. I just want to make you guys proud. </p>\n\n<p>Hoping the next project will be simpler - I ran into so many roadblocks with the Energy API and port forwarding on my own network, due to a conflict with pfsense and my access point that was still behaving as a router, apparently. </p>\n\n<p>Thanks in advance</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/xg4vqcffc12d1.png', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/xg4vqcffc12d1.png?auto=webp&s=f8230159c4e0c55eb8bb68659e29db88e6639c5f', 'width': 7490, 'height': 5519}, 'resolutions': [{'url': 'https://preview.redd.it/xg4vqcffc12d1.png?width=108&crop=smart&auto=webp&s=e4e6dbdeb4414a1a1fc0d6644a25390d205f351d', 'width': 108, 'height': 79}, {'url': 'https://preview.redd.it/xg4vqcffc12d1.png?width=216&crop=smart&auto=webp&s=e3a2bd9791638030906105ebd79fd00351bdd5d0', 'width': 216, 'height': 159}, {'url': 'https://preview.redd.it/xg4vqcffc12d1.png?width=320&crop=smart&auto=webp&s=dd51eec24b880336f38ddffd92a47da301d79dd6', 'width': 320, 'height': 235}, {'url': 'https://preview.redd.it/xg4vqcffc12d1.png?width=640&crop=smart&auto=webp&s=8b86d528fd29c1cd7624416b44cbb79547bbceb8', 'width': 640, 'height': 471}, {'url': 'https://preview.redd.it/xg4vqcffc12d1.png?width=960&crop=smart&auto=webp&s=4a4dd8c42e39f04f639845bc1fe5f9424cb1d8fb', 'width': 960, 'height': 707}, {'url': 'https://preview.redd.it/xg4vqcffc12d1.png?width=1080&crop=smart&auto=webp&s=b13fd0d179da7781715f393b8ec8fffafcacef7e', 'width': 1080, 'height': 795}], 'variants': {}, 'id': 'jCXZ8Zx_1V5AFGdor_DDBZJrpjQlZt6uPKQSYMOIz_0'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '4134b452-dc3b-11ec-a21a-0262096eec38', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ddbd37', 'id': '1cya2ch', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='pm_me_data_wisdom'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cya2ch/first_project_update_complete_few_questions/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://i.redd.it/xg4vqcffc12d1.png', 'subreddit_subscribers': 185337, 'created_utc': 1716408744.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.377+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_y3ldhmjb5', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "termpandas: scrollable pandas dataframes in the terminal. https://github.com/juan-esteban-berger/termpandas didn't find any tool like this so I decided to code one up myself, sharing it here in case somone else finds it useful, install using pip install termpandas", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 91, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cyihf8', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'ups': 24, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 24, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/Ogm-hdaWitmngesDp8Z3mhr-2_mwwFaRYHK8IUOZ7oY.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1716432469.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/tkdc2xcua32d1.gif', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?format=png8&s=4590eb0bc80f3f3476804f2e0e016e4feeaad87c', 'width': 2255, 'height': 1478}, 'resolutions': [{'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=108&crop=smart&format=png8&s=b94d7914ba9a121d69837fafa5352ea7e3f617d2', 'width': 108, 'height': 70}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=216&crop=smart&format=png8&s=036d700c68107a97b68e805a117fcbe245d65b83', 'width': 216, 'height': 141}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=320&crop=smart&format=png8&s=05dcfb4c3941ec8ed8784632dc5fc7823e999b01', 'width': 320, 'height': 209}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=640&crop=smart&format=png8&s=c3bd584a3b77ad7d9a0952e66853120c68463fe3', 'width': 640, 'height': 419}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=960&crop=smart&format=png8&s=c86e91b0229db0bfb144472ba57e8647b0cfb98b', 'width': 960, 'height': 629}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=1080&crop=smart&format=png8&s=7631047fa808049e30b5ab14097271c353005e46', 'width': 1080, 'height': 707}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?s=ea15884c95fc3da1ac7ee76558cbdfdf8164f3bf', 'width': 2255, 'height': 1478}, 'resolutions': [{'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=108&crop=smart&s=41a15cdc391e2e6b2b851d134a6bdc5a86359310', 'width': 108, 'height': 70}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=216&crop=smart&s=87bfff840b166ecbc2633601e8d964ca81f77472', 'width': 216, 'height': 141}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=320&crop=smart&s=a44abb7508a301a237800627b02815729311f7d1', 'width': 320, 'height': 209}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=640&crop=smart&s=6014d4aa371613a33c0f74f3988cb1f252d3ea03', 'width': 640, 'height': 419}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=960&crop=smart&s=a1aa881aef86d79caa80fe38fcd22660c22946f8', 'width': 960, 'height': 629}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=1080&crop=smart&s=df250f5ab62643b507a03e5bc5b47d0215dfc758', 'width': 1080, 'height': 707}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?format=mp4&s=bfeb83cfba339097e79f85cdeee3226bd5bbd371', 'width': 2255, 'height': 1478}, 'resolutions': [{'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=108&format=mp4&s=425b20aa2ef31e156fa94e7f8eb573b478c93313', 'width': 108, 'height': 70}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=216&format=mp4&s=db8f9f4c42128287c6b73ec54d2bde2c36a3571b', 'width': 216, 'height': 141}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=320&format=mp4&s=42f8bb3cf2002ed3b808c63e2115da7c6a591f79', 'width': 320, 'height': 209}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=640&format=mp4&s=67794a091c20878a5aed7e9bc4d85284354c844a', 'width': 640, 'height': 419}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=960&format=mp4&s=7d898bcf2fe583b16134d0cb794b964d557407bd', 'width': 960, 'height': 629}, {'url': 'https://preview.redd.it/tkdc2xcua32d1.gif?width=1080&format=mp4&s=b3fd1bacedc91099f8f5610e345a08ce2e40287b', 'width': 1080, 'height': 707}]}}, 'id': 'xE0HC6HPzMNpZewBnf_3aKZVJICumC-1H3s_ShvHvGM'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1cyihf8', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='juan_berger'), 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyihf8/termpandas_scrollable_pandas_dataframes_in_the/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://i.redd.it/tkdc2xcua32d1.gif', 'subreddit_subscribers': 185337, 'created_utc': 1716432469.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.377+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "We've recently implemented lifecycle policies for our S3 buckets. These policies weren't previously configured. How would someone go about thinking of best practices like Tags, Lifecycle Policies, Compression, etc if they wanted to start fresh.", 'author_fullname': 't2_3sqs3uub', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'THE Best Best Practices and Their Impact on Your Organization', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cyjok7', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 14, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 14, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716436454.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>We&#39;ve recently implemented lifecycle policies for our S3 buckets. These policies weren&#39;t previously configured. How would someone go about thinking of best practices like Tags, Lifecycle Policies, Compression, etc if they wanted to start fresh.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cyjok7', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='priyasweety1'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyjok7/the_best_best_practices_and_their_impact_on_your/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyjok7/the_best_best_practices_and_their_impact_on_your/', 'subreddit_subscribers': 185337, 'created_utc': 1716436454.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.377+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '[https://www.getorchestra.io/blog/how-i-use-gen-ai-as-a-data-engineer](https://www.getorchestra.io/blog/how-i-use-gen-ai-as-a-data-engineer)', 'author_fullname': 't2_voma7dkju', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Do you data engineering folks actually use Gen AI or nah', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cypmvq', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.77, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 16, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 16, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716460412.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p><a href="https://www.getorchestra.io/blog/how-i-use-gen-ai-as-a-data-engineer">https://www.getorchestra.io/blog/how-i-use-gen-ai-as-a-data-engineer</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/puiRUsf-gquiaZgEqw3Bx6uewpyUjACFOfjqYPN3_XI.jpg?auto=webp&s=eae44833e540c050e48d6c333f689b8ff31f67d4', 'width': 750, 'height': 500}, 'resolutions': [{'url': 'https://external-preview.redd.it/puiRUsf-gquiaZgEqw3Bx6uewpyUjACFOfjqYPN3_XI.jpg?width=108&crop=smart&auto=webp&s=8636934c587f38e716032e35bea35d444bf950c6', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/puiRUsf-gquiaZgEqw3Bx6uewpyUjACFOfjqYPN3_XI.jpg?width=216&crop=smart&auto=webp&s=70ae2b8562ea1b6ac76900e2316483d0931a520c', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/puiRUsf-gquiaZgEqw3Bx6uewpyUjACFOfjqYPN3_XI.jpg?width=320&crop=smart&auto=webp&s=0ba38bc791a31a21419927084bd5df9ca21f9164', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/puiRUsf-gquiaZgEqw3Bx6uewpyUjACFOfjqYPN3_XI.jpg?width=640&crop=smart&auto=webp&s=b2ccf9a0985c452b2fe73a6963ced40b76902944', 'width': 640, 'height': 426}], 'variants': {}, 'id': 'y4ZEyKIlQB0L6msrDysy9vqIo8Lpz7--ShFVsBFaKf8'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1cypmvq', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='engineer_of-sorts'), 'discussion_type': None, 'num_comments': 23, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cypmvq/do_you_data_engineering_folks_actually_use_gen_ai/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cypmvq/do_you_data_engineering_folks_actually_use_gen_ai/', 'subreddit_subscribers': 185337, 'created_utc': 1716460412.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.378+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I have thousands upon thousands of photos. They need to have some filter applied. Upon filter application they have to be uploaded to some storage, can be S3. This can be done with a Python script (using PIL), no biggie. I would like to orchestrate this somehow, i.e. if some photo processing fails for whatever reason I'd like to have that reported somewhere. Maybe see some progress bar? \n\nBasically, the steps are:\n\n\n    For each photo in source:\n    1. Download photo from source to local disk\n    2. Apply filter\n    3. Upload photo to destination store\n    4. Delete the local copy of the photo\n\nWhat would you recommend for my use case?\n\nThanks!", 'author_fullname': 't2_3cn0pmkd', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I have thousands of photos and I need to apply a filter to all of them. What tool to orchestrate this job?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cyaqz4', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1716445400.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716410522.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have thousands upon thousands of photos. They need to have some filter applied. Upon filter application they have to be uploaded to some storage, can be S3. This can be done with a Python script (using PIL), no biggie. I would like to orchestrate this somehow, i.e. if some photo processing fails for whatever reason I&#39;d like to have that reported somewhere. Maybe see some progress bar? </p>\n\n<p>Basically, the steps are:</p>\n\n<pre><code>For each photo in source:\n1. Download photo from source to local disk\n2. Apply filter\n3. Upload photo to destination store\n4. Delete the local copy of the photo\n</code></pre>\n\n<p>What would you recommend for my use case?</p>\n\n<p>Thanks!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cyaqz4', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='VcSv'), 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyaqz4/i_have_thousands_of_photos_and_i_need_to_apply_a/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyaqz4/i_have_thousands_of_photos_and_i_need_to_apply_a/', 'subreddit_subscribers': 185337, 'created_utc': 1716410522.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.378+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello all, we are currently running a GCP house in our company. We are using Bigquery as data warehouse and Cloud Composer for pipeline orchestration. \n\nWe do take advantage of the various Bigquery operators for parameterising/templating queries and to create tables if needed. \n\nMy question is, I have been reading a lot about DBT and how it can help us become more efficient. \n\nCan you  give me feedback/ideas on how it can help us data engineer or reduce our cloud costs? ', 'author_fullname': 't2_esibz', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'DBT needed?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cyp62o', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716458517.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello all, we are currently running a GCP house in our company. We are using Bigquery as data warehouse and Cloud Composer for pipeline orchestration. </p>\n\n<p>We do take advantage of the various Bigquery operators for parameterising/templating queries and to create tables if needed. </p>\n\n<p>My question is, I have been reading a lot about DBT and how it can help us become more efficient. </p>\n\n<p>Can you  give me feedback/ideas on how it can help us data engineer or reduce our cloud costs? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cyp62o', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ithellam_oru_pollapu'), 'discussion_type': None, 'num_comments': 19, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyp62o/dbt_needed/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyp62o/dbt_needed/', 'subreddit_subscribers': 185337, 'created_utc': 1716458517.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.378+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Title says it all. Our team is looking to deploy some dags on airflow but donâ€™t want to manage the setup. We have maybe 10 batch processes that must be run on a schedule.\n\nDoes anyone have experience with the was managed version? It looks pretty straightforward.\n', 'author_fullname': 't2_4ntkn6k8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Opinions on AWS Managed Airflow', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cy40vk', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716394135.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Title says it all. Our team is looking to deploy some dags on airflow but donâ€™t want to manage the setup. We have maybe 10 batch processes that must be run on a schedule.</p>\n\n<p>Does anyone have experience with the was managed version? It looks pretty straightforward.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cy40vk', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='tylerriccio8'), 'discussion_type': None, 'num_comments': 21, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cy40vk/opinions_on_aws_managed_airflow/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cy40vk/opinions_on_aws_managed_airflow/', 'subreddit_subscribers': 185337, 'created_utc': 1716394135.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.378+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Finally broke down and decided I should learn Polars. Been a few hiccups, but going well. One I ran into was this example:\n\nIn Pandas I have a function which splits a string every 8 characters, adds ".TIF" and returns it as a string that I can later convert to a list with ast\n\n`def  split_and_add_tif(text):`\n\n`return [\',\'.join([text[I:8] + \'.TIF\' for i in range(0, len(text, 8)])]`\n\nto apply this with Pandas I\'d simply do:\n\n`df[\'FileList\'] = df[\'FileList\'].apply(split_and_add_tif)`\n\nHowever in Polars it appears I need to do this:\n\n`df = df.with_columns(`\n\n`df[\'FileList\'].map_elements(split_and_add_tif, return_dtype = list).alias(\'FileList\')`\n\n`)`\n\nIs this the correct way to do this? Does anyone have a good resource for going from Pandas Functions to Polars? Still a bit confused on the difference between map\\_rows and map\\_elements. Any advice/pointers are welcomed.   \n', 'author_fullname': 't2_io9vf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is there a "correct" way to convert a Pandas Function to Polars?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cy59k9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716397155.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Finally broke down and decided I should learn Polars. Been a few hiccups, but going well. One I ran into was this example:</p>\n\n<p>In Pandas I have a function which splits a string every 8 characters, adds &quot;.TIF&quot; and returns it as a string that I can later convert to a list with ast</p>\n\n<p><code>def  split_and_add_tif(text):</code></p>\n\n<p><code>return [&#39;,&#39;.join([text[I:8] + &#39;.TIF&#39; for i in range(0, len(text, 8)])]</code></p>\n\n<p>to apply this with Pandas I&#39;d simply do:</p>\n\n<p><code>df[&#39;FileList&#39;] = df[&#39;FileList&#39;].apply(split_and_add_tif)</code></p>\n\n<p>However in Polars it appears I need to do this:</p>\n\n<p><code>df = df.with_columns(</code></p>\n\n<p><code>df[&#39;FileList&#39;].map_elements(split_and_add_tif, return_dtype = list).alias(&#39;FileList&#39;)</code></p>\n\n<p><code>)</code></p>\n\n<p>Is this the correct way to do this? Does anyone have a good resource for going from Pandas Functions to Polars? Still a bit confused on the difference between map_rows and map_elements. Any advice/pointers are welcomed.   </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cy59k9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Scalar_Mikeman'), 'discussion_type': None, 'num_comments': 13, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cy59k9/is_there_a_correct_way_to_convert_a_pandas/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cy59k9/is_there_a_correct_way_to_convert_a_pandas/', 'subreddit_subscribers': 185337, 'created_utc': 1716397155.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.378+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I was going to adopt data-diff as a tool to improve our CI CD in the next few months. My goal of using it is mostly to figure out whether the PR is going to drastically affect the row count, so I can take it as an alert for the developed feature.\n\nBut now that data-diff has been sunset, is there any similar tools out there that can solve the requirements above?\n\nThank you for reading.', 'author_fullname': 't2_tf5qdee5d', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Tool as a data-diff alternative?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cyn2a6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716449406.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I was going to adopt data-diff as a tool to improve our CI CD in the next few months. My goal of using it is mostly to figure out whether the PR is going to drastically affect the row count, so I can take it as an alert for the developed feature.</p>\n\n<p>But now that data-diff has been sunset, is there any similar tools out there that can solve the requirements above?</p>\n\n<p>Thank you for reading.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cyn2a6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SeaCompetitive5704'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyn2a6/tool_as_a_datadiff_alternative/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyn2a6/tool_as_a_datadiff_alternative/', 'subreddit_subscribers': 185337, 'created_utc': 1716449406.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.379+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Would love to hear your story in the comments.\n\n[View Poll](https://www.reddit.com/poll/1cy5un6)', 'author_fullname': 't2_lnwagoki', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How did you become a Data Engineer?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cy5un6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.87, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716398550.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Would love to hear your story in the comments.</p>\n\n<p><a href="https://www.reddit.com/poll/1cy5un6">View Poll</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cy5un6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AMDataLake'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'poll_data': <praw.models.reddit.poll.PollData object at 0x7f33c04350c0>, 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cy5un6/how_did_you_become_a_data_engineer/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'mod_reports': [], 'url': 'https://www.reddit.com/r/dataengineering/comments/1cy5un6/how_did_you_become_a_data_engineer/', 'subreddit_subscribers': 185337, 'created_utc': 1716398550.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.379+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi recently interviewed for a data modeling round wherein I had to design a relational transaction table (user purchasing the products which will be shown in the transaction table). I also had to design user table, product table, but for my question focus is only on the transaction table.\n\nI designed 2 tables for storing the transactions data -\n\n1. Transaction table itself with columns -\n\ntransactions\\_id (PK)\n\nuser\\_id (FK)\n\ntotal\\_amount\n\ndate\n\n2) Transaction\\_product (logs product level granularity information, separated for normalization)\n\nNote - There is a seperate product table, this table is just an extension of transaction table\n\ntransaction\\_id (FK)\n\nproduct\\_id (FK)\n\nprice\n\nquantity\n\nThe interviewer suggested that my design (having transaction\\_product as a separate table) might not be commonly used in applications and mentioned that the product\\_transactions table is behaving more like a fact table in my design. I didn't fully understand this feedback and would appreciate any advice on how to improve the design.\n\nIs there a more appropriate way to structure these tables for transaction data? If so, could you explain why the above schema might not be optimal and how it could be improved?\n\nThank you for your guidance.", 'author_fullname': 't2_bqiq0aqj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Modeling - Transaction Table Design', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cykpfi', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1716446366.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716440070.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi recently interviewed for a data modeling round wherein I had to design a relational transaction table (user purchasing the products which will be shown in the transaction table). I also had to design user table, product table, but for my question focus is only on the transaction table.</p>\n\n<p>I designed 2 tables for storing the transactions data -</p>\n\n<ol>\n<li>Transaction table itself with columns -</li>\n</ol>\n\n<p>transactions_id (PK)</p>\n\n<p>user_id (FK)</p>\n\n<p>total_amount</p>\n\n<p>date</p>\n\n<p>2) Transaction_product (logs product level granularity information, separated for normalization)</p>\n\n<p>Note - There is a seperate product table, this table is just an extension of transaction table</p>\n\n<p>transaction_id (FK)</p>\n\n<p>product_id (FK)</p>\n\n<p>price</p>\n\n<p>quantity</p>\n\n<p>The interviewer suggested that my design (having transaction_product as a separate table) might not be commonly used in applications and mentioned that the product_transactions table is behaving more like a fact table in my design. I didn&#39;t fully understand this feedback and would appreciate any advice on how to improve the design.</p>\n\n<p>Is there a more appropriate way to structure these tables for transaction data? If so, could you explain why the above schema might not be optimal and how it could be improved?</p>\n\n<p>Thank you for your guidance.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cykpfi', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Character-Tone-6952'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cykpfi/data_modeling_transaction_table_design/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cykpfi/data_modeling_transaction_table_design/', 'subreddit_subscribers': 185337, 'created_utc': 1716440070.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.379+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi everyone,\n\nI'm looking for some advice and best practices on implementing version control for creating and managing views in Snowflake. Currently, my team creates views directly in the Snowflake UI, but we want to move towards a more controlled and collaborative approach using version control.\n\nHas anyone implemented a similar setup or have recommendations on tools and workflows? How do you manage version control and deployment for Snowflake views in your organization?\n\nAny insights, experiences, or resources you could share would be greatly appreciated!\n\nThanks in advance!", 'author_fullname': 't2_zxx39gj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Seeking Advice on Implementing Version Control for Snowflake Views', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cy68ow', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716399508.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone,</p>\n\n<p>I&#39;m looking for some advice and best practices on implementing version control for creating and managing views in Snowflake. Currently, my team creates views directly in the Snowflake UI, but we want to move towards a more controlled and collaborative approach using version control.</p>\n\n<p>Has anyone implemented a similar setup or have recommendations on tools and workflows? How do you manage version control and deployment for Snowflake views in your organization?</p>\n\n<p>Any insights, experiences, or resources you could share would be greatly appreciated!</p>\n\n<p>Thanks in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cy68ow', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='djurisic_luka'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cy68ow/seeking_advice_on_implementing_version_control/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cy68ow/seeking_advice_on_implementing_version_control/', 'subreddit_subscribers': 185337, 'created_utc': 1716399508.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.379+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey all -- \n\nSo I work in a manufacturing / research setting, where we have constant streams of time series data coming off of "test stands" measuring things like temperature, etc.\n\nRight now we store it in an on-prem Postgres database using Timescale, with other information about tests that were performed in another, separate Postgres database.\n\nTo analyze data, scientists and engineers note the start and stop time of their test in that other Postgres database, with data being transformed and prepared whenever they download that "test". \n\nSince transforming data and doing calculations on data at the time of request has become expensive and somewhat slow, I want to store these chunks of processed time series data for consumption.\n\nCan anyone tell me the best way to do this? Ideally we\'d also like to be able to grab only parts of that chunk, as well.\n\nWould it be S3? Another relational database? I tried using Postgres JSONB columns on a whim, but insertion is super slow and there\'s the possibility of hitting the upper limit in terms of size.\n\nAppreciate any help!\n\n', 'author_fullname': 't2_o8rziw7x', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Best way to store chunks of time series data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1cyvj2g', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716478034.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey all -- </p>\n\n<p>So I work in a manufacturing / research setting, where we have constant streams of time series data coming off of &quot;test stands&quot; measuring things like temperature, etc.</p>\n\n<p>Right now we store it in an on-prem Postgres database using Timescale, with other information about tests that were performed in another, separate Postgres database.</p>\n\n<p>To analyze data, scientists and engineers note the start and stop time of their test in that other Postgres database, with data being transformed and prepared whenever they download that &quot;test&quot;. </p>\n\n<p>Since transforming data and doing calculations on data at the time of request has become expensive and somewhat slow, I want to store these chunks of processed time series data for consumption.</p>\n\n<p>Can anyone tell me the best way to do this? Ideally we&#39;d also like to be able to grab only parts of that chunk, as well.</p>\n\n<p>Would it be S3? Another relational database? I tried using Postgres JSONB columns on a whim, but insertion is super slow and there&#39;s the possibility of hitting the upper limit in terms of size.</p>\n\n<p>Appreciate any help!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cyvj2g', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='bad_specimen'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyvj2g/best_way_to_store_chunks_of_time_series_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyvj2g/best_way_to_store_chunks_of_time_series_data/', 'subreddit_subscribers': 185337, 'created_utc': 1716478034.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.379+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi everyone,\n\nI've been working with Databricks and exploring their Delta Live Tables (DLT) for pipeline orchestration and data engineering. However, Iâ€™ve hit a roadblock: it seems that DLT does not support external locations.\n\nIâ€™m curious to understand why this limitation exists. Are there technical reasons or specific considerations from Databricks' perspective that I might be missing? How do you work around this limitation in your projects?\n\nAny insights or experiences you can share would be greatly appreciated!\n\nThanks in advance!", 'author_fullname': 't2_utq8ecoj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "Why Doesn't Databricks DLT Support External Locations?", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cyty73', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716474033.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone,</p>\n\n<p>I&#39;ve been working with Databricks and exploring their Delta Live Tables (DLT) for pipeline orchestration and data engineering. However, Iâ€™ve hit a roadblock: it seems that DLT does not support external locations.</p>\n\n<p>Iâ€™m curious to understand why this limitation exists. Are there technical reasons or specific considerations from Databricks&#39; perspective that I might be missing? How do you work around this limitation in your projects?</p>\n\n<p>Any insights or experiences you can share would be greatly appreciated!</p>\n\n<p>Thanks in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cyty73', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='JazzlikeReaction631'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyty73/why_doesnt_databricks_dlt_support_external/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyty73/why_doesnt_databricks_dlt_support_external/', 'subreddit_subscribers': 185337, 'created_utc': 1716474033.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.379+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey there, I\'m new to the sub (after discovering what the thing I could never find out how to do properly as part of my Data Projects was called!). I am a student looking to build out projects end to end and gain experience with Open Source tools & Industry Best Practices. \n\nI wanted to hear peoples opinions or experiences regarding Local Compute / Storage options vs Cloud Free Tiers. I have approached Cloud before as partof different projects and it appeared quite intimidating, however I recently considered getting a few raspberry pis or old PCs that I could find lying around and "simulate" them as different components of the system. Like an RBP for Raw Data. An old laptop with CUDA to do the compute further down the line etc. Is this stupid and I should just go an use cloud free tiers, or could this be a worthwhile project for my learning (and a portfolio?). ', 'author_fullname': 't2_o8oegkoct', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Local Compute and Storage vs Cloud for Keen Student', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cypb40', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716459083.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey there, I&#39;m new to the sub (after discovering what the thing I could never find out how to do properly as part of my Data Projects was called!). I am a student looking to build out projects end to end and gain experience with Open Source tools &amp; Industry Best Practices. </p>\n\n<p>I wanted to hear peoples opinions or experiences regarding Local Compute / Storage options vs Cloud Free Tiers. I have approached Cloud before as partof different projects and it appeared quite intimidating, however I recently considered getting a few raspberry pis or old PCs that I could find lying around and &quot;simulate&quot; them as different components of the system. Like an RBP for Raw Data. An old laptop with CUDA to do the compute further down the line etc. Is this stupid and I should just go an use cloud free tiers, or could this be a worthwhile project for my learning (and a portfolio?). </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cypb40', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='CAOCDO'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cypb40/local_compute_and_storage_vs_cloud_for_keen/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cypb40/local_compute_and_storage_vs_cloud_for_keen/', 'subreddit_subscribers': 185337, 'created_utc': 1716459083.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.379+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi All, \n\nI am working in a project that uses DMS heavily, to pull CDC data from MS Sql Db to S3 in AWS itself.\n\nWe are then building a Deltalake on top for querying and usage.\n\nI am not very happy with DMS as its prone to failures, and looking for alternatives.\n\nIs there any other way to get this accomplished with out using DMS.\n\nThanks', 'author_fullname': 't2_da2fc3qr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Replacing AWS DMS', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cyfp7l', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1716425789.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716423795.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi All, </p>\n\n<p>I am working in a project that uses DMS heavily, to pull CDC data from MS Sql Db to S3 in AWS itself.</p>\n\n<p>We are then building a Deltalake on top for querying and usage.</p>\n\n<p>I am not very happy with DMS as its prone to failures, and looking for alternatives.</p>\n\n<p>Is there any other way to get this accomplished with out using DMS.</p>\n\n<p>Thanks</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cyfp7l', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SelectTraffic1104'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyfp7l/replacing_aws_dms/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyfp7l/replacing_aws_dms/', 'subreddit_subscribers': 185337, 'created_utc': 1716423795.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.380+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi!\n\nI am making a star schema for EuroLeague basketball data using [this](https://www.kaggle.com/datasets/babissamothrakis/euroleague-datasets) dataset. In one of the files, there is a column for three-letter team code (always the same) and team name (changes due to sponsorship reasons). \n\nI am not sure how to model these name changes. I plan to have a dimension Teams, and I am not sure if it is good practice to have several rows in the table which are the same team with different names? What would be other ways to approach the modeling? Please note that this is a university project, and I am only allowed to manipulate my data using SSIS.\n\nThanks in advance.', 'author_fullname': 't2_bhsuz6op', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Basketball club name changes: star schema', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cy9zr6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716408569.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi!</p>\n\n<p>I am making a star schema for EuroLeague basketball data using <a href="https://www.kaggle.com/datasets/babissamothrakis/euroleague-datasets">this</a> dataset. In one of the files, there is a column for three-letter team code (always the same) and team name (changes due to sponsorship reasons). </p>\n\n<p>I am not sure how to model these name changes. I plan to have a dimension Teams, and I am not sure if it is good practice to have several rows in the table which are the same team with different names? What would be other ways to approach the modeling? Please note that this is a university project, and I am only allowed to manipulate my data using SSIS.</p>\n\n<p>Thanks in advance.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/Bkiplc-XQotK9QSPZJNQbX1E-Z84VKMftOpJVWD4B1c.jpg?auto=webp&s=537ccdaf66d59a4194d64519491462867b7f3746', 'width': 1200, 'height': 1200}, 'resolutions': [{'url': 'https://external-preview.redd.it/Bkiplc-XQotK9QSPZJNQbX1E-Z84VKMftOpJVWD4B1c.jpg?width=108&crop=smart&auto=webp&s=8fb98ea4a0465fd7ee01ea3cec53b2e3b3d70ce5', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/Bkiplc-XQotK9QSPZJNQbX1E-Z84VKMftOpJVWD4B1c.jpg?width=216&crop=smart&auto=webp&s=d6673b9e75cd3cccfcd56b1a1bc9a0063eca964a', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/Bkiplc-XQotK9QSPZJNQbX1E-Z84VKMftOpJVWD4B1c.jpg?width=320&crop=smart&auto=webp&s=7b16a5a03deaca7a3e529bbe3571e3c641eaba42', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/Bkiplc-XQotK9QSPZJNQbX1E-Z84VKMftOpJVWD4B1c.jpg?width=640&crop=smart&auto=webp&s=83a7929853c7b3f23e921ee320676c2d984f9e99', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/Bkiplc-XQotK9QSPZJNQbX1E-Z84VKMftOpJVWD4B1c.jpg?width=960&crop=smart&auto=webp&s=cc1e895f4a307f19f103b13bbfb53415ac30a5b6', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/Bkiplc-XQotK9QSPZJNQbX1E-Z84VKMftOpJVWD4B1c.jpg?width=1080&crop=smart&auto=webp&s=4b08cb47d51c40c6b002687edcbf3c004d1771bd', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'xQSAAnjTVN-xai71HznwbP2hGdWgA8r4BZrf_A4uJ2I'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cy9zr6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Thauma-jpg'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cy9zr6/basketball_club_name_changes_star_schema/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cy9zr6/basketball_club_name_changes_star_schema/', 'subreddit_subscribers': 185337, 'created_utc': 1716408569.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.380+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_4js2c', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Real-time feature engineering', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 41, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cy6uli', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.63, 'author_flair_background_color': None, 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/WPiusqihs3GOiKdqJ8DaJVSdNLfHN4JSsfiyZAMvDN0.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1716400994.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'feldera.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.feldera.com/blog/feature-engineering-part2/', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/aD8DrsxXvK5n8HCgBqEjMtkNWPK5JSlnR1DMuZAgo6I.jpg?auto=webp&s=f34ba02b720fda985c13caf2a93b7d9fac6252d9', 'width': 1287, 'height': 380}, 'resolutions': [{'url': 'https://external-preview.redd.it/aD8DrsxXvK5n8HCgBqEjMtkNWPK5JSlnR1DMuZAgo6I.jpg?width=108&crop=smart&auto=webp&s=a8521d357a13f882226f0614e3fae4b9435bc5ac', 'width': 108, 'height': 31}, {'url': 'https://external-preview.redd.it/aD8DrsxXvK5n8HCgBqEjMtkNWPK5JSlnR1DMuZAgo6I.jpg?width=216&crop=smart&auto=webp&s=4918a6faf21c9d8729bbc71182fb417b504c3907', 'width': 216, 'height': 63}, {'url': 'https://external-preview.redd.it/aD8DrsxXvK5n8HCgBqEjMtkNWPK5JSlnR1DMuZAgo6I.jpg?width=320&crop=smart&auto=webp&s=d39df7b6058f006eec564d61ebbfba0320e69fee', 'width': 320, 'height': 94}, {'url': 'https://external-preview.redd.it/aD8DrsxXvK5n8HCgBqEjMtkNWPK5JSlnR1DMuZAgo6I.jpg?width=640&crop=smart&auto=webp&s=b755568c930858009bbb4bcccd429212e933d1f0', 'width': 640, 'height': 188}, {'url': 'https://external-preview.redd.it/aD8DrsxXvK5n8HCgBqEjMtkNWPK5JSlnR1DMuZAgo6I.jpg?width=960&crop=smart&auto=webp&s=9a0f5df77d1758b5642bafb681956bae7dde0f74', 'width': 960, 'height': 283}, {'url': 'https://external-preview.redd.it/aD8DrsxXvK5n8HCgBqEjMtkNWPK5JSlnR1DMuZAgo6I.jpg?width=1080&crop=smart&auto=webp&s=7f0e4e8fff105b3dc7d3d8dfeb3317de06f45653', 'width': 1080, 'height': 318}], 'variants': {}, 'id': 'r3LjToF3B9vgqKAPdOnFA-ZNeK-Rp44tmZWg9wo4fQk'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1cy6uli', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='mww09'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cy6uli/realtime_feature_engineering/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.feldera.com/blog/feature-engineering-part2/', 'subreddit_subscribers': 185337, 'created_utc': 1716400994.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.380+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Collected 5 book recs about real-time/streaming systems for those interested in diving deep into the domain: [https://www.arecadata.com/top-5-books-to-study-real-time-data-systems/](https://www.arecadata.com/top-5-books-to-study-real-time-data-systems/)\n\n', 'author_fullname': 't2_cqao8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '5 books to learn real-time systems', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1cyvi3o', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716477971.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Collected 5 book recs about real-time/streaming systems for those interested in diving deep into the domain: <a href="https://www.arecadata.com/top-5-books-to-study-real-time-data-systems/">https://www.arecadata.com/top-5-books-to-study-real-time-data-systems/</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/oIYIWuUozh1bva2OCzLuLKGNSELenzek7QlTZQKLpok.jpg?auto=webp&s=93a551ee1ff2e736a4b02e427e25ba0593499309', 'width': 1071, 'height': 683}, 'resolutions': [{'url': 'https://external-preview.redd.it/oIYIWuUozh1bva2OCzLuLKGNSELenzek7QlTZQKLpok.jpg?width=108&crop=smart&auto=webp&s=0f9a61dece70f965cb79fd9fad4da6725b4a1396', 'width': 108, 'height': 68}, {'url': 'https://external-preview.redd.it/oIYIWuUozh1bva2OCzLuLKGNSELenzek7QlTZQKLpok.jpg?width=216&crop=smart&auto=webp&s=f0e448bda9d868a171353e4066e79dd72385a299', 'width': 216, 'height': 137}, {'url': 'https://external-preview.redd.it/oIYIWuUozh1bva2OCzLuLKGNSELenzek7QlTZQKLpok.jpg?width=320&crop=smart&auto=webp&s=99903a210e84038aca14a2a1a342227de979e95a', 'width': 320, 'height': 204}, {'url': 'https://external-preview.redd.it/oIYIWuUozh1bva2OCzLuLKGNSELenzek7QlTZQKLpok.jpg?width=640&crop=smart&auto=webp&s=a6e1b8f52dba13d2e2c14d87fb7e57f5213547c1', 'width': 640, 'height': 408}, {'url': 'https://external-preview.redd.it/oIYIWuUozh1bva2OCzLuLKGNSELenzek7QlTZQKLpok.jpg?width=960&crop=smart&auto=webp&s=1a8fb2f4b4d956489a266ccd03375359cf78220a', 'width': 960, 'height': 612}], 'variants': {}, 'id': 'mcVhpxKhiQ14moph36EepMTPfhbbIHL5hdV-E8lyiLE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1cyvi3o', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='dan_the_lion'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyvi3o/5_books_to_learn_realtime_systems/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyvi3o/5_books_to_learn_realtime_systems/', 'subreddit_subscribers': 185337, 'created_utc': 1716477971.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.380+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am working  in 2 projects where am mainly working as  L2 application support engineer  for a java based application and secondary  responsible is monitoring ETL process scheduled runs in ADF pipelines. Am now planning to move my career from application support to Data Engineer.Since, i have some beginner level familiarity with adf & etl process. Could you guys assist me from where and what topics should i learnand gain knowledge , so i can switch to my next role as data engineer( currently only having knowledge in sql and python without hands-on experience)', 'author_fullname': 't2_jty17j0d', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Need Assistance in Career Transition', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1cyvevv', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716477740.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am working  in 2 projects where am mainly working as  L2 application support engineer  for a java based application and secondary  responsible is monitoring ETL process scheduled runs in ADF pipelines. Am now planning to move my career from application support to Data Engineer.Since, i have some beginner level familiarity with adf &amp; etl process. Could you guys assist me from where and what topics should i learnand gain knowledge , so i can switch to my next role as data engineer( currently only having knowledge in sql and python without hands-on experience)</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cyvevv', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='HelpfulGeologist5007'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyvevv/need_assistance_in_career_transition/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyvevv/need_assistance_in_career_transition/', 'subreddit_subscribers': 185337, 'created_utc': 1716477740.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.381+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I am working on a project where I scrape data from multiple websites, using a separate \\`.py\\` script for each website. Each script varies certain parameters to perform 'incremental scrapes' while keeping other parameters fixed. For example,\n\n- Website A, I vary parameters like \\`DATE\\_FROM\\` and \\`DATE\\_TO\\` while keeping \\`TITLE\\`, \\`RATING\\`, and \\`PAGE\\_SIZE\\` fixed.\n\n- Website B, I vary \\`YEAR\\` and \\`RATING\\`, and keep \\`TITLE\\` and \\`MAX\\_RESULTS\\` fixed.\n\n- Website C, I don't vary any parameters.\n\n\n\n(In reality there are 50+ sites I am doing this for.)\n\n\n\nThe results are dumped into separate directories for each website (\\`data/website\\_A/\\`, \\`data/website\\_B/\\`, etc.). Currently, I name the output files to include the varied parameters, which has resulted in complex and hard-to-read filenames. For example, in \\`data/website\\_A/\\`, I have filenames like:\n\n\n\n1. \\`1800-01-01\\_2002-01-01\\_loop\\_2002-01-02\\_2005-01-01.csv\\`\n\n2. \\`loop\\_2005-01-02\\_2010-01-01.csv\\`\n\n\n\nwhich is meant to indicate \n\n\n\n1. the scraper runs on \\`DATE\\_FROM = 1800-01-01\\` and \\`DATE\\_TO = 2002-01-01\\`, followed by a loop from \\`DATE\\_FROM = DATE\\_TO = 2002-01-02\\` to \\`DATE\\_FROM = DATE\\_TO = 2005-01-01\\`,\n\n\n\n2. the scraper runs on a loop from \\`DATE\\_FROM = DATE\\_TO = 2005-01-02\\` to \\`DATE\\_FROM = DATE\\_TO = 2010-01-01\\`\n\n\n\nbut from these filenames, it's not immediately clear which parameters were varied and which were kept fixed. \n\n\n\nMy questions are:\n\n\n\n1. What is the best practice for organizing these folders and files so that it's easy to identify which parameters were varied without cluttering the filenames?\n\n2. Is there a recommended way to log the parameters, both varied and fixed, for each scrape in a clear and maintainable manner?\n\n\n\nIt seems like I will have to end up keeping parameters in a separate file, but I'm not sure how to best implement this. Any suggestions on how to effectively manage and document these scrapes would be greatly appreciated.", 'author_fullname': 't2_s9545lpak', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Best Practices for Organizing and Documenting Parameterized Scraper Output Files', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cyotne', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716457059.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am working on a project where I scrape data from multiple websites, using a separate `.py` script for each website. Each script varies certain parameters to perform &#39;incremental scrapes&#39; while keeping other parameters fixed. For example,</p>\n\n<ul>\n<li><p>Website A, I vary parameters like `DATE_FROM` and `DATE_TO` while keeping `TITLE`, `RATING`, and `PAGE_SIZE` fixed.</p></li>\n<li><p>Website B, I vary `YEAR` and `RATING`, and keep `TITLE` and `MAX_RESULTS` fixed.</p></li>\n<li><p>Website C, I don&#39;t vary any parameters.</p></li>\n</ul>\n\n<p>(In reality there are 50+ sites I am doing this for.)</p>\n\n<p>The results are dumped into separate directories for each website (`data/website_A/`, `data/website_B/`, etc.). Currently, I name the output files to include the varied parameters, which has resulted in complex and hard-to-read filenames. For example, in `data/website_A/`, I have filenames like:</p>\n\n<ol>\n<li><p>`1800-01-01_2002-01-01_loop_2002-01-02_2005-01-01.csv`</p></li>\n<li><p>`loop_2005-01-02_2010-01-01.csv`</p></li>\n</ol>\n\n<p>which is meant to indicate </p>\n\n<ol>\n<li><p>the scraper runs on `DATE_FROM = 1800-01-01` and `DATE_TO = 2002-01-01`, followed by a loop from `DATE_FROM = DATE_TO = 2002-01-02` to `DATE_FROM = DATE_TO = 2005-01-01`,</p></li>\n<li><p>the scraper runs on a loop from `DATE_FROM = DATE_TO = 2005-01-02` to `DATE_FROM = DATE_TO = 2010-01-01`</p></li>\n</ol>\n\n<p>but from these filenames, it&#39;s not immediately clear which parameters were varied and which were kept fixed. </p>\n\n<p>My questions are:</p>\n\n<ol>\n<li><p>What is the best practice for organizing these folders and files so that it&#39;s easy to identify which parameters were varied without cluttering the filenames?</p></li>\n<li><p>Is there a recommended way to log the parameters, both varied and fixed, for each scrape in a clear and maintainable manner?</p></li>\n</ol>\n\n<p>It seems like I will have to end up keeping parameters in a separate file, but I&#39;m not sure how to best implement this. Any suggestions on how to effectively manage and document these scrapes would be greatly appreciated.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cyotne', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='bebmfec'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyotne/best_practices_for_organizing_and_documenting/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyotne/best_practices_for_organizing_and_documenting/', 'subreddit_subscribers': 185337, 'created_utc': 1716457059.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.381+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'We have a SaaS vendor who will make data available via a replica Database(AWS RDS Postgres) only, we need to get this data(a replica of the replica). Looked into ETLs tools like Fivetran, Hevo but the limitation seems to lie with Postgres not allowing logical replication of a replica DB. Access to WAL might not be an option to explore. Looks like a typical scenario where a SaaS vendor would not expose anything other than a replica but the data integration solution does not seem simple though. What is the best way to replicate such a replica DB?', 'author_fullname': 't2_mepyumy4q', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "Data integration method from a SaaS vendor's replica database", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cyjjfg', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716435960.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>We have a SaaS vendor who will make data available via a replica Database(AWS RDS Postgres) only, we need to get this data(a replica of the replica). Looked into ETLs tools like Fivetran, Hevo but the limitation seems to lie with Postgres not allowing logical replication of a replica DB. Access to WAL might not be an option to explore. Looks like a typical scenario where a SaaS vendor would not expose anything other than a replica but the data integration solution does not seem simple though. What is the best way to replicate such a replica DB?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cyjjfg', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Emrgnt24'), 'discussion_type': None, 'num_comments': 0, 'send_replies': False, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyjjfg/data_integration_method_from_a_saas_vendors/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyjjfg/data_integration_method_from_a_saas_vendors/', 'subreddit_subscribers': 185337, 'created_utc': 1716435960.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.381+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm diving into DevOps theory and I was thinking about cycle times for centralized vs decentralized data teams.\n\nMy understanding is that decentralizing the creation and maintenance of data products reduces cycle time because the people who produce (or at least understand) the data are close to the people who create the data product.\n\nOn the other hand, I can imagine that, as a data consumer, if you need improvements on existing data products, it can be a hassle to get in touch with the right team and get your request prioritized.\n\nData engineers that work in a Data Mesh (-ish) environment, is this something you recognize? Or is it still better than making requests to a centralized team?\n", 'author_fullname': 't2_5q8r885o', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Mesh caveat?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cy9b22', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716406913.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m diving into DevOps theory and I was thinking about cycle times for centralized vs decentralized data teams.</p>\n\n<p>My understanding is that decentralizing the creation and maintenance of data products reduces cycle time because the people who produce (or at least understand) the data are close to the people who create the data product.</p>\n\n<p>On the other hand, I can imagine that, as a data consumer, if you need improvements on existing data products, it can be a hassle to get in touch with the right team and get your request prioritized.</p>\n\n<p>Data engineers that work in a Data Mesh (-ish) environment, is this something you recognize? Or is it still better than making requests to a centralized team?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cy9b22', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='scataco'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cy9b22/data_mesh_caveat/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cy9b22/data_mesh_caveat/', 'subreddit_subscribers': 185337, 'created_utc': 1716406913.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.382+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm getting a ton of targeted ads after downloading their O'Reilly guide. Does anyone actually use any of their services? Outside of cost, it's not clear to me how they would be a better choice over Snowflake.\n\n*Apologies if this is a super ignorant question, I'm fairly new to DE, and in a non-technical role", 'author_fullname': 't2_10wit0e9m3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is Dremio a viable product?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cy3zxw', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716394067.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m getting a ton of targeted ads after downloading their O&#39;Reilly guide. Does anyone actually use any of their services? Outside of cost, it&#39;s not clear to me how they would be a better choice over Snowflake.</p>\n\n<p>*Apologies if this is a super ignorant question, I&#39;m fairly new to DE, and in a non-technical role</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cy3zxw', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nolake2003'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cy3zxw/is_dremio_a_viable_product/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cy3zxw/is_dremio_a_viable_product/', 'subreddit_subscribers': 185337, 'created_utc': 1716394067.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.382+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Is there a way to run dbt commands in the production environment w/out scheduling jobs?  ', 'author_fullname': 't2_70oyd0wh', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Manually run dbt build in production?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cy70rd', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716401404.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Is there a way to run dbt commands in the production environment w/out scheduling jobs?  </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cy70rd', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='FragrantData8875'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cy70rd/manually_run_dbt_build_in_production/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cy70rd/manually_run_dbt_build_in_production/', 'subreddit_subscribers': 185337, 'created_utc': 1716401404.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.382+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi All,  \nWe're excited to share that we've launched a DoubleCloud Managed Apache Airflow service designed to make your data workflow automation a breeze. Hereâ€™s what you can expect:\n\n \n\n* **Managed infrastructure**: We take care of cluster creation and updates so you can focus on your pipelines.\n* **Auto-Scaling**: No need to tweak settings for performance. Our worker nodes auto-scale based on your needs.\n* **Clear Insights**: Get easy access to logs and notifications with our user-friendly UI.\n* **Customizable**: Bring your own plugins, packages, or libraries by customizing the default container image.\n* **Rapid DAG Development**: Start quickly with pre-packaged common libraries and integrate them with your GIT setup effortlessly.\n* **Secure Environment**: Our workers and schedulers run in a secure setup.\n* **Integration**: Works smoothly with other DoubleCloud services to manage end-to-end data flows.  \n\n\n Find out all about it on our new service page: [https://double.cloud/services/managed-airflow/](https://double.cloud/services/managed-airflow/)  \nWeâ€™re also offering a little something extra. If you try out our service and give us feedback, we'll send you a $50 Amazon voucher. Your input will help us fine-tune this service to better meet your needs.  \n\n\nJust drop me a message to discuss the details.  \n ", 'author_fullname': 't2_a4qx2du', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'New Managed Apache Airflow service', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cyrek3', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.4, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1716466776.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi All,<br/>\nWe&#39;re excited to share that we&#39;ve launched a DoubleCloud Managed Apache Airflow service designed to make your data workflow automation a breeze. Hereâ€™s what you can expect:</p>\n\n<ul>\n<li><strong>Managed infrastructure</strong>: We take care of cluster creation and updates so you can focus on your pipelines.</li>\n<li><strong>Auto-Scaling</strong>: No need to tweak settings for performance. Our worker nodes auto-scale based on your needs.</li>\n<li><strong>Clear Insights</strong>: Get easy access to logs and notifications with our user-friendly UI.</li>\n<li><strong>Customizable</strong>: Bring your own plugins, packages, or libraries by customizing the default container image.</li>\n<li><strong>Rapid DAG Development</strong>: Start quickly with pre-packaged common libraries and integrate them with your GIT setup effortlessly.</li>\n<li><strong>Secure Environment</strong>: Our workers and schedulers run in a secure setup.</li>\n<li><p><strong>Integration</strong>: Works smoothly with other DoubleCloud services to manage end-to-end data flows.  </p>\n\n<p>Find out all about it on our new service page: <a href="https://double.cloud/services/managed-airflow/">https://double.cloud/services/managed-airflow/</a><br/>\nWeâ€™re also offering a little something extra. If you try out our service and give us feedback, we&#39;ll send you a $50 Amazon voucher. Your input will help us fine-tune this service to better meet your needs.  </p></li>\n</ul>\n\n<p>Just drop me a message to discuss the details.  </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/GHt1D6qhyqOLT9rNa08ayli01dcMM3P0o4RjwrXOO_M.jpg?auto=webp&s=a91f4a2774bc8ee2146a3e34515b1fbab8e68dd6', 'width': 2400, 'height': 1256}, 'resolutions': [{'url': 'https://external-preview.redd.it/GHt1D6qhyqOLT9rNa08ayli01dcMM3P0o4RjwrXOO_M.jpg?width=108&crop=smart&auto=webp&s=ce241b921995a82aa92c9b4d823d7a76765c3ef0', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/GHt1D6qhyqOLT9rNa08ayli01dcMM3P0o4RjwrXOO_M.jpg?width=216&crop=smart&auto=webp&s=b41bbf1dbccf4fe6f28da7885f9def621ae60fb9', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/GHt1D6qhyqOLT9rNa08ayli01dcMM3P0o4RjwrXOO_M.jpg?width=320&crop=smart&auto=webp&s=8ccbc0a8e3af20473656b7c0fbe3d296832a2403', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/GHt1D6qhyqOLT9rNa08ayli01dcMM3P0o4RjwrXOO_M.jpg?width=640&crop=smart&auto=webp&s=6b228181b54845f13b0a515dbd8070acfe5bf24b', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/GHt1D6qhyqOLT9rNa08ayli01dcMM3P0o4RjwrXOO_M.jpg?width=960&crop=smart&auto=webp&s=c327106399b4215a5deb899f344d16e500ea10ff', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/GHt1D6qhyqOLT9rNa08ayli01dcMM3P0o4RjwrXOO_M.jpg?width=1080&crop=smart&auto=webp&s=0af3120c770e9cc6b8d3e6c32462b96a71119d79', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'BcGF7W2uOTHwpzkas0xp0BBOs5g-O5wmsEbtKIPCFkc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1cyrek3', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='deepanigi'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cyrek3/new_managed_apache_airflow_service/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cyrek3/new_managed_apache_airflow_service/', 'subreddit_subscribers': 185337, 'created_utc': 1716466776.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.382+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f33c03db520>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '[https://www.youtube.com/watch?v=bge4MPHnkno](https://www.youtube.com/watch?v=bge4MPHnkno)\n\nI managed to link Taylor Swift ðŸ’ƒ into a sentence with Snowflake â„ï¸ Also - Snowflake Copilot ðŸ§‘\u200dâœˆï¸ will ease your job\xa0Business\xa0Analyst or Data Engineer.\xa0See why in my latest video where I also used new editing tricks. Less hassle, more information.\n\nhttps://reddit.com/link/1cypebj/video/u1gm4685j52d1/player\n\nVideo contents for the busy ones:\n\nðŸ“– 00:00 Intro  \nâ„ï¸ 00:28 What is Snowflake Copilot?  \nðŸ’¬ 01:37 Differences compared to ChatGPT?  \nðŸ’ƒ 02:23 Does Copilot know Taylor Swift?  \nðŸ’Ž 03:27 What (other) benefits against the competition?  \nâŒ 04:23 (current) limitations - part 1  \nâŒ 05:25 (current) limitations - part 2  \nðŸ–‡ï¸ 06:22 Cross Database queries  \nðŸŒ¬ï¸ 07:06 Supported languages  \nðŸ’² 07:47 Pricing  \nðŸŽ¯ 08:07 Wrapping it up  \nðŸ”® 08:36 Looking into the future', 'author_fullname': 't2_sxd6cnuh', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What is Snowflake Copilot and does it know Taylor Swift?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 105, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'u1gm4685j52d1': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/1cypebj/asset/u1gm4685j52d1/DASHPlaylist.mpd?a=1719074073%2CZTVlNGRiNTVlMzQ5ZjQ5ODJhY2QyZDMzZmE5ZWE2ZWVhZjI2ZGEyNmUyNzJiNWQ0YmZkNjExMjVmNzE2Y2JkNQ%3D%3D&v=1&f=sd', 'x': 1920, 'y': 1080, 'hlsUrl': 'https://v.redd.it/link/1cypebj/asset/u1gm4685j52d1/HLSPlaylist.m3u8?a=1719074073%2CNTZmMDAyOGNjZmM4YzhlODZkM2Q1ZWI0NDkyNWZkNDFjZmY3ZjEwYzU2YzQyMzFhN2I1ZjlkNzNjZWI5NzIzZA%3D%3D&v=1&f=sd', 'id': 'u1gm4685j52d1', 'isGif': False}}, 'name': 't3_1cypebj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.36, 'author_flair_background_color': None, 'ups': 0, 'total_awards_received': 0, 'media_embed': {'content': '<iframe width="356" height="200" src="https://www.youtube.com/embed/bge4MPHnkno?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="What is Snowflake COPILOT and does it know Taylor Swift?"></iframe>', 'width': 356, 'scrolling': False, 'height': 200}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'What is Snowflake COPILOT and does it know Taylor Swift?', 'html': '<iframe width="356" height="200" src="https://www.youtube.com/embed/bge4MPHnkno?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="What is Snowflake COPILOT and does it know Taylor Swift?"></iframe>', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': 'Mika Heino', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/bge4MPHnkno/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/@MikaHeino'}, 'type': 'youtube.com'}, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {'content': '<iframe width="356" height="200" src="https://www.youtube.com/embed/bge4MPHnkno?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="What is Snowflake COPILOT and does it know Taylor Swift?"></iframe>', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/1cypebj', 'height': 200}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/G5hNyjIsl-sgh8E6Qt0CfpiVvxrhBzOf_ceR6Td7HL8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'subreddit_type': 'public', 'created': 1716459467.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p><a href="https://www.youtube.com/watch?v=bge4MPHnkno">https://www.youtube.com/watch?v=bge4MPHnkno</a></p>\n\n<p>I managed to link Taylor Swift ðŸ’ƒ into a sentence with Snowflake â„ï¸ Also - Snowflake Copilot ðŸ§‘\u200dâœˆï¸ will ease your job\xa0Business\xa0Analyst or Data Engineer.\xa0See why in my latest video where I also used new editing tricks. Less hassle, more information.</p>\n\n<p><a href="https://reddit.com/link/1cypebj/video/u1gm4685j52d1/player">https://reddit.com/link/1cypebj/video/u1gm4685j52d1/player</a></p>\n\n<p>Video contents for the busy ones:</p>\n\n<p>ðŸ“– 00:00 Intro<br/>\nâ„ï¸ 00:28 What is Snowflake Copilot?<br/>\nðŸ’¬ 01:37 Differences compared to ChatGPT?<br/>\nðŸ’ƒ 02:23 Does Copilot know Taylor Swift?<br/>\nðŸ’Ž 03:27 What (other) benefits against the competition?<br/>\nâŒ 04:23 (current) limitations - part 1<br/>\nâŒ 05:25 (current) limitations - part 2<br/>\nðŸ–‡ï¸ 06:22 Cross Database queries<br/>\nðŸŒ¬ï¸ 07:06 Supported languages<br/>\nðŸ’² 07:47 Pricing<br/>\nðŸŽ¯ 08:07 Wrapping it up<br/>\nðŸ”® 08:36 Looking into the future</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/BtQLnlUXYEkbnGrQoIJF7Yr3i2sC-XkbGX0T6KTTbj0.jpg?auto=webp&s=735c8c3af10d591353cc0d4c5eea5541090f461b', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/BtQLnlUXYEkbnGrQoIJF7Yr3i2sC-XkbGX0T6KTTbj0.jpg?width=108&crop=smart&auto=webp&s=680286805aaaa01da452e009dfaf9f9e1a1d72c1', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/BtQLnlUXYEkbnGrQoIJF7Yr3i2sC-XkbGX0T6KTTbj0.jpg?width=216&crop=smart&auto=webp&s=6d68e9b4b7a6f1d047f9a1c9eb5833491dbc195e', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/BtQLnlUXYEkbnGrQoIJF7Yr3i2sC-XkbGX0T6KTTbj0.jpg?width=320&crop=smart&auto=webp&s=ee55a62a62c735013ff331dd33976dc1b964e7f4', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'DAB6_5sTjiCWjyzXH-KJYpa09wNvSr8u4uyZCrYQ02w'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1cypebj', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Recordly_MHeino'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cypebj/what_is_snowflake_copilot_and_does_it_know_taylor/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cypebj/what_is_snowflake_copilot_and_does_it_know_taylor/', 'subreddit_subscribers': 185337, 'created_utc': 1716459467.0, 'num_crossposts': 0, 'media': {'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'What is Snowflake COPILOT and does it know Taylor Swift?', 'html': '<iframe width="356" height="200" src="https://www.youtube.com/embed/bge4MPHnkno?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="What is Snowflake COPILOT and does it know Taylor Swift?"></iframe>', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'version': '1.0', 'author_name': 'Mika Heino', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/bge4MPHnkno/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/@MikaHeino'}, 'type': 'youtube.com'}, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-23T16:34:34.383+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-05-23T16:34:34.383+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-05-23T16:34:34.399+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, run_id=manual__2024-05-23T16:34:31.927050+00:00, execution_date=20240523T163431, start_date=20240523T163433, end_date=20240523T163434
[2024-05-23T16:34:34.437+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-05-23T16:34:34.451+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-05-23T16:34:34.453+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
